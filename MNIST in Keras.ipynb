{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "nb_classes = 10\n",
    "np.random.seed(2017) # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data\n",
    "The neural-network is going to take a single vector for each training example, so reshaping the input so that each 28x28 image becomes a single 784 dimensional vector is done. Scale the inputs to be in the range [0-1] rather than [0-255] so that we give all features of the input vector same importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPrep(conv):\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    print(\"X_train original shape\", X_train.shape)\n",
    "    print(\"y_train original shape\", y_train.shape)\n",
    "    \n",
    "    if conv:\n",
    "            X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "            X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "    else:\n",
    "        X_train = X_train.reshape(60000, 784)\n",
    "        X_test = X_test.reshape(10000, 784)\n",
    "    \n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "    \n",
    "    # One-hot encoding the output i.e 1-->(1,0,0,0,0,0,0,0,0,0),2-->(0,1,0,0,0,0,0,0,0,0),....\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    \n",
    "    print(\"Training matrix shape\", X_train.shape)\n",
    "    print(\"Training target matrix shape\", Y_train.shape)\n",
    "    print(\"Testing matrix shape\", X_test.shape)\n",
    "    print(\"Testing target matrix shape\", Y_test.shape)\n",
    "    \n",
    "    return X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the neural network\n",
    "A simple 3 layer fully connected network.\n",
    "<img src=\"plots/figure.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "The loss function here is **categorical crossentropy**, a well-suited to comparing two probability distributions.\n",
    "\n",
    "Here our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,Y_train,X_test,Y_test,drop_prob,activ_fun,num_epochs,callback):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(512, input_shape=(784,))) # Hidden layer_1 with 512 neurons\n",
    "    model.add(Activation(activ_fun)) \n",
    "    model.add(Dropout(drop_prob))             # Dropout regularization to avoid overfitting\n",
    "    model.add(Dense(512))                     # Hidden layer_2 with 512 neurons \n",
    "    model.add(Activation(activ_fun))\n",
    "    model.add(Dropout(drop_prob)) \n",
    "    model.add(Dense(10))                      # Output Layer with 10 neurons \n",
    "    model.add(Activation('softmax')) \n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    if callback:\n",
    "        earlyStopping=EarlyStopping(monitor='loss', min_delta=0.0001, patience=5, verbose=0, mode='auto')\n",
    "        model_info = model.fit(X_train, Y_train,callbacks=[earlyStopping],batch_size=128, \n",
    "                               epochs=num_epochs,verbose=2,validation_split=0.15)\n",
    "    else:\n",
    "        model_info = model.fit(X_train, Y_train,batch_size=128,epochs=num_epochs,verbose=2,validation_split=0.15)\n",
    "    score = model.evaluate(X_test, Y_test,verbose=0)\n",
    "    return model_info,score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('X_train original shape', (60000, 28, 28))\n",
      "('y_train original shape', (60000,))\n",
      "('Training matrix shape', (60000, 784))\n",
      "('Training target matrix shape', (60000, 10))\n",
      "('Testing matrix shape', (10000, 784))\n",
      "('Testing target matrix shape', (10000, 10))\n",
      "Train on 51000 samples, validate on 9000 samples\n",
      "Epoch 1/4\n",
      " - 12s - loss: 0.2695 - acc: 0.9173 - val_loss: 0.1132 - val_acc: 0.9677\n",
      "Epoch 2/4\n",
      " - 11s - loss: 0.1085 - acc: 0.9667 - val_loss: 0.0824 - val_acc: 0.9756\n",
      "Epoch 3/4\n",
      " - 11s - loss: 0.0782 - acc: 0.9757 - val_loss: 0.0741 - val_acc: 0.9786\n",
      "Epoch 4/4\n",
      " - 11s - loss: 0.0592 - acc: 0.9814 - val_loss: 0.0822 - val_acc: 0.9761\n",
      "('Test score:', 0.079624776807846506)\n",
      "('Test accuracy:', 0.97489999999999999)\n"
     ]
    }
   ],
   "source": [
    "conv = False\n",
    "callback = False\n",
    "drop_prob = 0.2\n",
    "activ_fun = 'relu'\n",
    "num_epochs = 4\n",
    "X_train,Y_train,X_test,Y_test = dataPrep(conv)\n",
    "if conv:\n",
    "    print 'No model'\n",
    "else:\n",
    "    model_info,score = train(X_train,Y_train,X_test,Y_test,drop_prob,activ_fun,num_epochs,callback)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying dropout -- [0.1,0.2,0.4]  \n",
    "\n",
    "with *relu* as Activation function in all the instances\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src='plots/drop_0.1.png'></td>\n",
    "        <td><img src='plots/drop_0.2.png'></td>\n",
    "        <td><img src='plots/drop_0.4.png'></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "| DropOut Ratio | train_loss*| test_accuracy(%)\n",
    "|:---|:---:  |---: |\n",
    "|no| 0.0102|97.78|\n",
    "|0.1 | 0.0197|98.09|\n",
    "|0.2 | 0.0258|98.20|\n",
    "|0.4 | 0.0466|98.03|\n",
    "\n",
    "<center><sup>* - after training for 10 epochs</sup></center>\n",
    "\n",
    "### Inferences:\n",
    "* The train_loss increases with the dropout ratio increasing due to information lost  i.e, as more neurons are dropped their contribution to the decrement of error is temporally removed on the on the backward pass and any weight updates are not applied to those neurons.\n",
    "* Test accuracy increase till 0.2 and it drops for 0.4 ratio.\n",
    "* When you increase dropout beyond a certain threshold, it results in the model not being able to fit properly. \n",
    "* Dropout is like all other forms of regularization in that it reduces model capacity. If you reduce the capacity too much, it is sure that you will get bad results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation fuctions -- [sigmoid,relu,tanh]\n",
    "Drop_ratio = 0.2\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src='plots/sigmoid.png'></td>\n",
    "        <td><img src='plots/relu.png'></td>\n",
    "        <td><img src='plots/tanh.png'></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "| Activation function | train_loss* | test_accuracy(%)\n",
    "|:---|:---:  |---: |\n",
    "|sigmoid | 0.0598|97.76|\n",
    "|relu | 0.0253|98.12|\n",
    "|tanh | 0.0380|97.82|\n",
    "<center><sup>* - after training for 10 epochs</sup></center>\n",
    "\n",
    "\n",
    "### Inferences:\n",
    "* The train_loss increases with the dropout ratio increasing due to information lost  i.e, as more neurons are dropped their contribution to the decrement of error is temporally removed on the on the backward pass and any weight updates are not applied to those neurons.\n",
    "* Test accuracy increase till 0.2 and it drops for 0.4 ratio.\n",
    "* When you increase dropout beyond a certain threshold, it results in the model not being able to fit properly. \n",
    "* Dropout is like all other forms of regularization in that it reduces model capacity. If you reduce the capacity too much, it is sure that you will get bad results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No.of epochs\n",
    "<center>Drop_Ratio = 0.2, Activation fuction is *relu* in both the instances </center>\n",
    "\n",
    "<center>Training for 100 epochs</center>\n",
    "<table>\n",
    "        <tr><td><img src=\"plots/epochs_100.png\", style=\"height: 250px;\" ></tr></td>\n",
    "</table>\n",
    "\n",
    "<center>Training stopped at 35 epochs with Early Stopping</center>\n",
    "<table>\n",
    "    <tr><td><img src=\"plots/earlystopping.png\", style=\"height: 250px;\"></tr></td>\n",
    "</table>\n",
    "\n",
    "| No.of epochs | Early Stopping | test_accuracy(%)\n",
    "|:---|:---:|---: |\n",
    "|100|No |98.18|\n",
    "|35|Yes |98.39|\n",
    "\n",
    "### Inferences:\n",
    "* When trained for 100 epochs the test accuracy is 98.18% where as if the training is stopped at 35 epochs using early stopping with criteria that if there is no change in train_loss then stop the training, the accuracy increases to 98.39%. This decrease in accuracy for the first model is may be due to overfitting as the validation loss increases(1st plot) after certain epochs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lenet model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
